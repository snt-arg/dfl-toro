{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DFL-TORO","text":"<p>DFL-TORO is a one-shot demonstration framework for learning time-optimal robotic manufacturing tasks. It optimizes Learning from Demonstration (LfD) by minimizing the need for multiple demonstrations, resulting in efficient, noise-free, and jerk-regulated trajectories. Designed for intuitive kinesthetic demonstrations, DFL-TORO enhances robotic programming efficiency and precision in tasks such as pick-and-place. Evaluations with robots like the Franka Emika Research 3 (FR3) and ABB YuMi demonstrate its effectiveness in transforming demonstrations into optimized, high-performance manufacturing solutions. Video</p> <p></p> <p></p>"},{"location":"#modules-and-packages","title":"Modules and Packages","text":""},{"location":"#principal-modules","title":"Principal Modules","text":"<p>The principal modules comprising DFL-TORO are briefly described in the following table:</p> Module Description lfd_interface The LFD Interface module is the core element of DFL-TORO. This module is responsible for providing an interface to various modules of the software and allowing interaction of the user with different functionalities of the software. The main design objective in developing this module was to provide a standard interface to enhance the extensibility and modularity of the framework. lfd_smoothing The main role of the this module is to take raw recorded demonstrations and convert them into optimal demonstration trajectories. lfd_dmp This module is the implementation of Dynamic Movement Primitives (DMPs), which is responsible for training demonstrations and planning trajectories based on the required start and goal configuration. <p>Besides the principal packages, several other packages are included to enable implementation on FR3 and ABB Dual-Arm YuMi.</p>"},{"location":"#fr3-modules","title":"FR3 Modules","text":"Module Description franka_ws Launch files and required config to run different controllers on FR3. fr3_moveit_config Moveit configuration package for FR3. franka_drake Description package to enable using FR3 model in PyDrake."},{"location":"#abb-yumi-modules","title":"ABB YuMi Modules","text":"Module Description yumi_bringup Source files and scripts required to control ABB Yumi via the ABB Robot Driver yumi_moveit_config Moveit configuration package for ABB Yumi. yumi_drake Description package to enable using Yumi model in PyDrake."},{"location":"#notes","title":"Notes","text":"<p>Please be aware that the included packages are intended for academic use and have not undergone productization. They are provided \"as-is,\" with only limited support available.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the SnT Academic License- see the LICENSE for more details.</p>"},{"location":"#contributions","title":"Contributions","text":"<p>Contributions are welcome! If you have any suggestions, bug reports, or feature requests, please create a new issue or pull request.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was supported by the Luxembourg National Research Fund (FNR) through the Project \u2018\u2018A Combined Machine Learning Approach for the Engineering of Flexible Assembly Processes Using Collaborative Robots (ML-COBOTS)\u2019\u2019 under Grant 15882013.</p> <p>If you use this framework in your scientific research, we would appreciate if you cite the corresponding paper: <pre><code>@article{barekatain2024dfl,\n  title={Dfl-toro: A one-shot demonstration framework for learning time-optimal robotic manufacturing tasks},\n  author={Barekatain, Alireza and Habibi, Hamed and Voos, Holger},\n  journal={IEEE Access},\n  year={2024},\n  publisher={IEEE}\n}\n</code></pre></p>"},{"location":"#maintainers","title":"Maintainers","text":"<ul> <li>Alireza Barekatain</li> </ul>"},{"location":"1.install/","title":"Installation Guide","text":""},{"location":"1.install/#docker-installation","title":"Docker Installation","text":""},{"location":"1.install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux distribution (Tested with Ubuntu 20.04)</li> <li>Docker and Docker Compose installed</li> <li>vcs tools</li> </ul>"},{"location":"1.install/#installation-steps","title":"Installation Steps","text":"<ol> <li>Clone the Repository</li> </ol> <p>Begin by cloning the repository and navigating to the project directory:</p> <pre><code>git clone https://github.com/snt-arg/dfl-toro.git\ncd dfl-toro\n</code></pre> <ol> <li>Import Required Repositories</li> </ol> <p>Import the required repositories into a mount folder using the following commands:</p> <pre><code>mkdir -p _ws/src\nvcs import --recursive _ws/src &lt; .rosinstall_dfl_toro\n# In case of using FR3 robot\nvcs import --recursive _ws/src &lt; .rosinstall_franka\n# In case of using ABB Yumi\nvcs import --recursive _ws/src &lt; .rosinstall_abb_yumi\n</code></pre> <ol> <li>Run the Docker Container</li> </ol> <p>Move to the Docker configuration directory and start the container:</p> <pre><code>cd docker/compose\ndocker compose up\n</code></pre> <ol> <li>Access the Docker Container</li> </ol> <p>Once the container is running, access it via the following command:</p> <pre><code>docker exec -it ros_noetic bash\ncd ~/main_ws/\n</code></pre>"},{"location":"1.install/#building-from-source","title":"Building from Source","text":""},{"location":"1.install/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>ROS Noetic (Tested only on Ubuntu 20.04)</li> <li>Moveit1</li> <li>CMake</li> <li>vcs tools</li> </ul>"},{"location":"1.install/#installation-steps_1","title":"Installation Steps","text":"<ol> <li>Install System Dependencies</li> </ol> <p>Ensure your system is up to date and install the necessary dependencies:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y \\\n      python3-vcstool \\\n      ros-noetic-ros-control \\\n      ros-noetic-ros-controllers \\\n      ros-noetic-actionlib-tools\n</code></pre> <ol> <li>Clone the Repository</li> </ol> <p>Clone the repository and navigate to the project directory:</p> <pre><code>git clone https://github.com/snt-arg/dfl-toro.git\ncd dfl-toro\n</code></pre> <ol> <li>Install Python Dependencies</li> </ol> <p>Install all the required Python packages listed in the requirements file:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Import Required Repositories</li> </ol> <p>Use vcs to import the required repositories:</p> <pre><code>vcs import --recursive $YOUR_CATKIN_WS_DIR$/src &lt; .rosinstall_dfl_toro\n# In case of using FR3 robot\nvcs import --recursive $YOUR_CATKIN_WS_DIR$/src &lt; .rosinstall_franka\n# In case of using ABB Yumi\nvcs import --recursive $YOUR_CATKIN_WS_DIR$/src &lt; .rosinstall_abb_yumi\n</code></pre> <ol> <li>Install Package Dependencies</li> </ol> <p>Install dependencies for the ROS packages:</p> <pre><code>cd $YOUR_CATKIN_WS_DIR$\nrosdep init &amp;&amp; rosdep update --include-eol-distros\nrosdep install --from-paths src --ignore-src --rosdistro noetic\n</code></pre> <ol> <li>Build the Packages</li> </ol> <p>Build the packages using the <code>catkin</code> command:</p> <pre><code>catkin build\n</code></pre>"},{"location":"Usage/","title":"Usage Guide","text":"<p>This tutorial is designed for the FR3 robotic arm, utilizing a learning approach based on DMP. Before proceeding, please create the following folder structure within the <code>lfd_interface</code> package to organize the necessary data:</p> <pre><code>&lt;lfd_interface&gt;/\n\u251c\u2500\u2500 data                     \n    \u251c\u2500\u2500 demonstrations \n    \u2514\u2500\u2500 smoother\n        \u251c\u2500\u2500 timing\n        \u251c\u2500\u2500 waypoints\n        \u2514\u2500\u2500 traj\n</code></pre> <p>A video demonstration for a pick and place task is available here</p>"},{"location":"Usage/1.record/","title":"Record a Demonstration","text":"<p>To record a demonstration, use the following command:</p> <pre><code>roslaunch lfd_interface lfd_recorder.launch config:=fr3 name:=DEMO_NAME\n</code></pre> <p>Note: The demonstration name must not contain numbers or special characters such as underscores.</p> <ul> <li>When recording starts, the recorder will automatically ignore the initial part of the recording where the robot is stationary and has not yet moved (i.e., before the demonstration begins).</li> <li>During the recording, the recorded path will be visualized in real-time using RViz.</li> </ul> <p>To stop the recording, press <code>Ctrl+C</code>. The demonstration will be saved under <code>lfd_interface/data/demonstrations/</code>.</p>"},{"location":"Usage/1.record/#what-is-recorded","title":"What is Recorded","text":"<p>The recorded data includes the joint states, their timings, and the end-effector pose throughout the demonstration, with a specified recording frequency.</p>"},{"location":"Usage/1.record/#configuration","title":"Configuration","text":"<p>The configuration file is located at <code>lfd_interface/config/demonstrations</code>. The configuration file for FR3 looks like this:</p> <pre><code>robot_ns: \"fr3\"\nplanning_group: \"fr3_arm\"\nbase_frame: \"fr3_link0\"\nee_frame: \"fr3_hand_tcp\"\n</code></pre> <ul> <li>robot_ns: Specifies the robot's namespace used for topics and services.</li> <li>planning_group: Defines the MoveIt planning group to be used during the demonstration.</li> <li>base_frame: The name of the link in the robot's URDF file that serves as the base frame.</li> <li>ee_frame: The name of the link in the robot's URDF file that serves as the end-effector frame, used for recording the end-effector pose throughout the demonstration.</li> </ul>"},{"location":"Usage/2.optimize/","title":"Optimize a Demonstration","text":"<p>The optimization process refines the original demonstration trajectory to produce a noise-free, smooth, and efficient trajectory.</p> <pre><code>roslaunch lfd_smoothing trajectory_smoothing.launch demo_name:=DEMO_NAME robot_type:=fr3\n</code></pre> <p>The optimization occurs in two stages. At the end of each stage, a plot of the resulting trajectory will be displayed. Close the first plot to proceed to the second stage. The optimized demonstration will be saved with the prefix \"smooth\" (e.g., if the original demonstration is named \"pick\", the optimized version will be named \"smoothpick\").</p> <p>The result can be visualized through Drake's MeshCat, accessible by default at <code>localhost:7000</code>.</p>"},{"location":"Usage/2.optimize/#configuration","title":"Configuration","text":"<p>The configuration file for optimization is located in <code>lfd_smoothing/config</code>. The general configuration file for FR3 looks like this:</p> <pre><code>smoother_config: {\n  demo_filter_threshold: 0.01,\n  pkg_xml: $(find lfd_smoothing)/drake/franka_drake/package.xml,\n  urdf_path: package://franka_drake/urdf/fr3_nohand.urdf,\n  config_hierarchy: [\"$(find lfd_smoothing)/config/opt/initial.yaml\", \"$(find lfd_smoothing)/config/opt/main.yaml\"],\n  robot_type: \"fr3\"\n}\n</code></pre> <ul> <li>demo_filter_threshold: Determines the minimum distance between waypoints extracted from the original path. A higher value reduces the number of waypoints, leading to faster optimization.</li> <li>pkg_xml and urdf_path: Paths for the robot's URDF packages, used by Drake to model and visualize the robot.</li> <li>config_hierarchy: Paths to configuration files for the two stages of optimization.</li> </ul>"},{"location":"Usage/2.optimize/#optimization-stages","title":"Optimization Stages","text":"<p>The configuration files for each stage of the optimization are in <code>lfd_smoothing/config/opt</code>.</p>"},{"location":"Usage/2.optimize/#first-optimization-stage-configuration-fr3","title":"First Optimization Stage Configuration (FR3)","text":"<pre><code>num_cps: 4\nbspline_order: 4\nvelocity_scaling: 1\nduration_bound: [0.01, 5]\ncoeff_duration: 1\ntol_joint: 0\n</code></pre> <ul> <li>num_cps: Number of control points per path segment.</li> <li>bspline_order: Order of the B-spline used for trajectory modeling.</li> <li>velocity_scaling: Scaling factor for velocity limits (default is 1).</li> <li>duration_bound: Minimum and maximum duration bounds for the trajectory.</li> <li>coeff_duration: Coefficient for the duration term in the cost function.</li> <li>tol_joint: Tolerance for deviation from the original joint configuration.</li> </ul>"},{"location":"Usage/2.optimize/#second-optimization-stage-configuration-fr3","title":"Second Optimization Stage Configuration (FR3)","text":"<pre><code>bspline_order: 4\nvelocity_scaling: 1\nacceleration_scaling: 1\njerk_scaling: 1\nduration_bound: [0.01, 5]\ncoeff_duration: 1\ncoeff_jerk: 0.04\ncoeff_joint_cp_error: 1\ntol_translation: 0.02\ntol_rotation: 0.05\n</code></pre> <ul> <li>acceleration_scaling and jerk_scaling: Similar to velocity scaling, used to adjust limits for acceleration and jerk.</li> <li>coeff_jerk: Coefficient for the jerk term in the cost function.</li> <li>coeff_joint_cp_error: Coefficient for penalizing deviations from the original joint configuration.</li> <li>tol_translation: Tolerance for end-effector translational deviation.</li> <li>tol_rotation: Tolerance for end-effector rotational deviation.</li> </ul>"},{"location":"Usage/2.optimize/#tunable-parameters","title":"Tunable Parameters","text":"<p>Most configuration parameters are suitable for general use, but the following can be adjusted for specific demonstrations:</p> <ul> <li>demo_filter_threshold: Increasing this value allows for a smoother, faster trajectory by reducing the number of waypoints.</li> <li>tol_translation and tol_rotation: Adjust these values based on the required accuracy. Higher tolerances provide more freedom for smoother, faster optimization.</li> </ul>"},{"location":"Usage/3.lfd/","title":"Learning from a Demonstration","text":"<p>To initiate the LfD process, use the following command:</p> <pre><code>roslaunch lfd_interface program_bundle.launch robot_group:=fr3\n</code></pre> <p>This command launches several components, including the IK solver server, the MoveIt utility node, and the DMP server, to perform the LfD process.</p>"},{"location":"Usage/3.lfd/#available-topics-and-actions","title":"Available Topics and Actions","text":"<p>The launched system exposes the following topics and actions:</p> <ul> <li> <p><code>fr3/lfd_pipeline</code> Action Server: Allows training on a specific demonstration and planning an LfD for a new goal configuration with a defined duration scale. It also provides options to visualize the resulting plan in RViz or directly execute it on the robot.</p> </li> <li> <p><code>fr3/plan_joint</code> and <code>fr3/plan_pose</code> Action Servers: These servers use MoveIt's internal motion planning to plan and execute trajectories to reach a specified joint configuration or end-effector pose. The <code>fr3/plan_pose</code> server also requires an initial joint configuration for IK, considering null space in the case of redundant manipulators.</p> </li> <li> <p><code>fr3/pose_state</code> Topic: Publishes the pose of the end-effector, similar to how the <code>joint_states</code> topic publishes joint positions.</p> </li> </ul> <p>The topics and actions provided allow for flexibility in LfD, including planning, visualization, and real-time execution, facilitating effective robot programming through demonstrations.</p> <p> </p>"},{"location":"Usage/4.program/","title":"Crafting LfD Programs with High-Level Instructions","text":"<p>To create complete programs from demonstrated subtasks, high-level instructions can be used to combine LfD-based subtasks into comprehensive robotic tasks.</p>"},{"location":"Usage/4.program/#example-pick-and-place-task","title":"Example: Pick and Place Task","text":"<p>Below is an example Python script that demonstrates how to craft an LfD program for a pick-and-place task using high-level instructions:</p> <pre><code>import rospy\nfrom lfd_program.runner import ProgramRunner\n\nif __name__ == \"__main__\":\n\n    rospy.init_node(\"lfd_program\", anonymous=True)\n\n    # Initialize the program\n    runner = ProgramRunner(robot=\"fr3\")\n    runner.set_motion_mode(\"dmp\")\n    runner.configure_motion(duration_scale=DURATION_SCALE)\n    runner.set_camera()  # If a camera program is set up\n\n    # Picking LfD subtask\n    runner.configure_motion(demo_name=\"PICK_DEMO_NAME\")\n    runner.move(debug=True)\n    runner.locate_target(\"object_alias\")  # If a camera program is set up and the object alias is defined\n    runner.gripper.gripper_grasp()\n\n    # Placing LfD subtask\n    runner.configure_motion(demo_name=\"PLACE_DEMO_NAME\")\n    runner.move(debug=True)\n    runner.gripper.gripper_open()\n</code></pre>"},{"location":"Usage/4.program/#debug-mode","title":"Debug Mode","text":"<p>If debug mode is activated, the planned LfD trajectory is first visualized in RViz before being executed on the robot. This helps verify the planned motion to ensure it meets the desired outcome.</p>"}]}